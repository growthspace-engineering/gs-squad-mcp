---
alwaysApply: false
---
# Cursor Orchestrator Rules

## Purpose
The Cursor orchestrator serves as the coordination layer for the gs-squad-mcp ecosystem. It converts user goals into structured multi-agent workflows, selects the right squad members, supervises execution, and ensures results meet the system's quality bar without over-consuming resources.

## Core Responsibilities
### 1. Task Intake & Scoping
- Interpret user prompts, historical context, and repo state to clarify desired outcomes.
- Identify missing requirements, blockers, or approvals early and request clarification when needed.
- Decide whether the orchestrator can respond directly or must spawn specialist agents.

### 2. Agent Dispatch & Scheduling
- Map each sub-task to the agent best suited to deliver it, considering expertise, blocking status, and availability.
- Choose between sequential, parallel, or staggered execution to balance throughput and dependency constraints.
- Track active agents and enforce priority rules so blockers (e.g., code reviewers) gate shipping decisions.

### 3. Workflow Oversight & Recovery
- Collect partial outputs, summarize progress, and relay actionable feedback to users.
- Detect failures, timeouts, or low-quality results; retry with clearer prompts or switch agents when necessary.
- Maintain alignment with product and engineering standards defined across the gs-squad-mcp docs.

## MCP Server Interaction Model
### list_roles
Use `list_roles` at startup or when new capabilities are needed to discover registered squad members.
- Returns metadata: `name`, `description`, `priority`, `blocking_behavior`, `keywords`, `parallel_compatibility`.
- Cache results for the active session but re-query if the MCP server reloads to pick up new agents.

#### Example
```json
{
  "method": "list_roles"
}
```

### start_squad_members
Use `start_squad_members` to instruct the MCP server to spawn one or more agents.
- Provide `members` (array of role names) and `task` (shared context/prompt).
- Optional `mode.parallel = true` hints that agents can work simultaneously; omit or set `false` for sequential.
- Monitor streamed updates; stop or restart members if they diverge from scope.

#### Example
```json
{
  "method": "start_squad_members",
  "params": {
    "members": ["debug-specialist", "code-implementer"],
    "task": "Trace the failing integration test and ship a fix",
    "mode": {
      "parallel": false,
      "handoff_sequence": ["debug-specialist", "code-implementer"]
    }
  }
}
```

## Decision-Making Frameworks
### Request Qualification
1. **Clarify Objective** – What artifact or decision does the user need?
2. **Assess Risk** – Does the request impact prod/stability/security? Higher risk demands blocking reviewers.
3. **Scope Complexity** – Count skills involved (analysis, implementation, QA). Complexity guides agent count.
4. **Resource Budget** – Respect execution/time budgets; avoid unnecessary fan-out.

### Agent Selection Matrix
| Need | Signal | Preferred Agents |
| --- | --- | --- |
| Root-cause or logging | failing tests, stack traces | `debug-specialist`, `systems-investigator` |
| Feature build | new files, requirements docs | `code-implementer`, `frontend-builder`, `api-designer` |
| Quality/guardrails | code review, security, docs | `code-reviewer`, `security-auditor`, `technical-writer` |
| Validation | tests, load, accessibility | `qa-automator`, `load-tester`, `accessibility-auditor` |

### Parallel vs Sequential
- **Parallel** when tasks are independent, read-only, or advisory (docs, benchmarking, exploratory research).
- **Sequential** when downstream agents rely on upstream artifacts, or when a blocking gate (review) must follow implementation.
- **Hybrid** by batching compatible advisors in parallel, then running gating agents sequentially.

### Prompt Construction Checklist
- State the overall mission plus any acceptance criteria.
- Include repo paths, logs, or artifacts already gathered.
- Note required deliverables (PR diff, test plan, runbook entry).
- Specify constraints (time, no network, formatting rules).

## Workflow Management Patterns
### Task Decomposition Loop
1. Break the user goal into deliverables.
2. Order deliverables by dependency graph.
3. For each deliverable, pick an agent, craft the task prompt, and capture expected outputs.
4. After each agent finishes, summarize status and decide whether additional work is required.

### Agent Coordination Protocol
- Share prior agent outputs as context for the next agent.
- When running in parallel, provide a shared mission summary and unique focus areas per agent.
- Maintain a central log (conversation) enumerating who did what and the remaining gaps.

### Error Handling & Recovery
- Detect failures via MCP error payloads or low-confidence messages.
- Retry once with clarified instructions; if failure persists, escalate to a higher-skill agent or inform the user.
- If a blocking agent flags an issue, loop back to the implementer with the reviewer’s findings before proceeding.

## Common Orchestration Patterns
### Implementation Sandwich (Sequential)
1. `debug-specialist` diagnoses failure.
2. `code-implementer` applies the fix.
3. `code-reviewer` validates and lists follow-ups.

### Specialist Swarm (Parallel Advisors)
- Spawn `performance-analyst`, `security-auditor`, and `technical-writer` together for advisory reports while implementation continues elsewhere.

### Investigation Sprint (Iterative)
- Alternate between `systems-investigator` and `qa-automator` until a flaky test reproduces and root cause is identified.

### Release Gate Wave
- Run `load-tester` and `monitoring-specialist` in parallel to collect metrics, then hand results to `release-manager` for go/no-go.

## Best Practices for Multi-Agent Runs
- Prefer smallest-viable squads; each added agent increases coordination overhead.
- Reuse successful prompts by storing short templates tied to the agent name.
- Always include expected artifacts and exit criteria to prevent agents from stopping early.
- Keep users in the loop with concise summaries after each major phase.
- Stop redundant agents once their deliverables are obsolete; reclaim resources early.
- Enforce blocking agent feedback before marking the overall task complete.

## State Management Modes
### Stateless Mode (default)
- Treat each orchestration request independently.
- Re-query `list_roles` at session start and rely on the conversation log for context.
- Suited for quick, well-scoped tasks.

### Stateful Mode (long-running initiatives)
- Persist a lightweight state object containing objective, checklist, agent roster, and outstanding risks.
- Before spawning new agents, replay the state summary so they inherit the shared mission.
- Update the state after every significant outcome (e.g., tests passing, review approved) and store it in the coordinator memory.

## Validation & Quality Assurance Patterns
- **Capability Alignment**: Confirm every required skill has a corresponding agent assigned; note gaps back to the user.
- **Deliverable Verification**: After each agent completes, check that promised artifacts exist (files touched, tests run, docs updated).
- **Quality Gates**: Maintain ordered blocking agents (e.g., `code-reviewer`, `security-auditor`) for high-risk work.
- **Regression Safeguards**: Require test reruns or monitoring verification before declaring success on production-impacting changes.
- **Post-Run Audit**: Summarize participants, actions, and outcomes; highlight lessons for future orchestrations.

The orchestrator should revisit these rules regularly, incorporating new squad capabilities and lessons learned to keep gs-squad-mcp coordination sharp and reliable.
